<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Shenandoah garbage collection in OpenJDK 16: Concurrent reference processing</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/kLbotXu8hOI/shenandoah-garbage-collection-openjdk-16-concurrent-reference-processing" /><author><name>Roman Kennke</name></author><id>d9c065a8-086a-41f0-97b0-c321b82bf650</id><updated>2021-05-20T07:00:00Z</updated><published>2021-05-20T07:00:00Z</published><summary type="html">&lt;p&gt;The primary motivation behind the &lt;a href="https://openjdk.java.net/projects/shenandoah/"&gt;Shenandoah garbage collection (GC) project&lt;/a&gt; in the OpenJDK was to reduce garbage collection pause times. Reference processing has traditionally been one of the primary contributors to GC pauses. The relationship is mostly linear: The more references the application is churning, the higher is the impact on garbage collection pauses and latency. The key here is "churning," or how many references need to be processed at every GC cycle. The references with referents that never die, or that die along with references themselves, are not a problem.&lt;/p&gt; &lt;p&gt;I have myself recommended in the past that if you care about latency, you had better not churn soft, weak, and phantom references or finalizees. In this article, I want to show why reference processing has contributed to &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; garbage collection pauses in the past, and how we solved that problem by making reference processing concurrent in JDK 16.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: If your application churns through soft, weak, or phantom references or finalizees, JDK 16 with its concurrent reference processing in Shenandoah GC might significantly improve your application's latency.&lt;/p&gt; &lt;h2&gt;References recap&lt;/h2&gt; &lt;p&gt;The term &lt;em&gt;references&lt;/em&gt; in this article means Java objects of type &lt;code&gt;java.lang.ref.Reference&lt;/code&gt; and its subtypes &lt;code&gt;SoftReference&lt;/code&gt;, &lt;code&gt;WeakReference&lt;/code&gt;, &lt;code&gt;PhantomReference&lt;/code&gt;, and &lt;code&gt;FinalReference&lt;/code&gt; (more on the last one later). Regular references between objects are also called &lt;em&gt;strong references&lt;/em&gt;. Each reference points to one referent. The purpose of the various reference types is to be able to reference an object, but not keep that object from being reclaimed by the reference. Reclamation follows reachability rules, which are specified roughly as follows (in order of decreasing reachability):&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;SoftReference&lt;/code&gt;: The referent can be reclaimed as soon as it is no longer reachable by any strong reference. Reclamation is additionally subject to other heuristics; e.g., usually soft references are not expected to be reclaimed unless memory pressure is high. This makes soft references suitable for memory-sensitive (cache) implementations. A rather bad choice, but a choice nonetheless: GC would clear soft references when memory is tight.&lt;/li&gt; &lt;li&gt;&lt;code&gt;WeakReference&lt;/code&gt;: The referent can be reclaimed as soon as it is no longer reachable by any strong or soft reference. As long as it has not been reclaimed, the referent may still be accessed, at which point it becomes strongly reachable again.&lt;/li&gt; &lt;li&gt;&lt;code&gt;FinalReference&lt;/code&gt;: You probably don't know this one, because it is a JDK-internal reference type. It is used to implement &lt;code&gt;Object.finalize()&lt;/code&gt;. Any object that implements &lt;code&gt;finalize()&lt;/code&gt; becomes the referent of a &lt;code&gt;FinalReference&lt;/code&gt; and is registered in a corresponding &lt;code&gt;ReferenceQueue&lt;/code&gt;. As soon as that object is no longer reachable by strong, soft, or weak references, it is processed and its &lt;code&gt;finalize()&lt;/code&gt; method is called.&lt;/li&gt; &lt;li&gt;&lt;code&gt;PhantomReference&lt;/code&gt;: The referent may be reclaimed as soon as it is no longer reachable by any strong, soft, weak, or final reference; in other words, when it is properly unreachable. The referent can never be accessed. This is done so that the referent cannot be accidentally resurrected after it has been determined to be unreachable. As soon as the reachability of referents has been determined, unreachable referents get cleared (set to null) and their corresponding reference objects get enqueued in a respective &lt;code&gt;ReferenceQueue&lt;/code&gt; for further processing by the application. Phantom references are typically used for managing resources such as native memory or operating system file handles that could otherwise not be handled by GC.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Traditional reference processing&lt;/h2&gt; &lt;p&gt;In versions prior to JDK 16, Shenandoah discovers and processes references in a way that closely follows the reachability rules in the previous section:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;First we determine the reachability of all strongly reachable objects during concurrent marking. As soon as the marking wavefront reaches a reference object that does not still have a marked referent (i.e., an object that is not strongly reachable from somewhere else), it stops there, and enqueues the reference in one of four &lt;em&gt;discovery queues&lt;/em&gt; for later processing. There is one discovery queue per reference type. A special case concerns soft references: the GC may decide, based on heuristics (e.g., memory pressure and/or object age), whether to treat soft references like strong references.&lt;/li&gt; &lt;li&gt;Next, as soon as concurrent (strong) marking is complete, the JVM stops the application and starts processing the discovery queues: &lt;ol&gt;&lt;li&gt;All &lt;code&gt;SoftReference&lt;/code&gt; objects are inspected. If the referent is not strongly reachable (i.e., marked by concurrent marking), it is cleared, and the &lt;code&gt;SoftReference&lt;/code&gt; is put in the processing queue.&lt;/li&gt; &lt;li&gt;All &lt;code&gt;WeakReference&lt;/code&gt; objects are inspected. If the referent is not strongly reachable, it is cleared, and the &lt;code&gt;WeakReference&lt;/code&gt; is put in the processing queue.&lt;/li&gt; &lt;li&gt;All &lt;code&gt;FinalReference&lt;/code&gt; objects are inspected. If the referent is not strongly reachable, the &lt;code&gt;FinalReference&lt;/code&gt; is put in the processing queue, but the referent is not yet cleared, because it is still needed later so that the GC can call its &lt;code&gt;finalize()&lt;/code&gt; method. Also, something special happens now: starting from the otherwise unreachable referent, marking is resumed, and the subgraph of objects found from the referent is marked. This is important in the next step, to avoid reclaiming &lt;code&gt;PhantomReference&lt;/code&gt; objects that are reachable from finalizees. This additional marking pass is problematic because it happens while the JVM is stopped, and it is theoretically bounded only by the live data set size. In other words, we may spend a lot of time here marking the subgraph from finalizees.&lt;/li&gt; &lt;li&gt;All &lt;code&gt;PhantomReference&lt;/code&gt; objects are inspected. If the referent is not reachable (neither strongly nor from finalizees), the referent is cleared, and the &lt;code&gt;PhantomReference&lt;/code&gt; is put in the processing queue.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Finally, the processing queue is added to a Java linked list for further &lt;code&gt;ReferenceQueue&lt;/code&gt; processing.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;From these steps, it follows that the contribution of reference processing to GC pause time is basically proportional to &lt;em&gt;number-of-processed-references + size-of-newly-marked-subgraph&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;In order to address the pause-time problem of reference processing, we need the GC to do it concurrently. The task of reference processing can be divided into two subtasks: the concurrent marking of referents, including their respective subgraphs, and the concurrent processing and clearing of references. Those two subtasks are entangled in the traditional implementation, so let's see how we can untangle them.&lt;/p&gt; &lt;h3&gt;Concurrent reference marking&lt;/h3&gt; &lt;p&gt;Upon closer inspection of the traditional implementation, we find that we can simplify our reachability model. We don't really have five levels of reachability (strong, soft, weak, final, phantom), but only two. Let's look at the criteria for classifying references from a different angle:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;SoftReference&lt;/code&gt; objects get cleared and enqueued when the referent is not strongly reachable and we meet a certain heuristic. In essence, we can decide, before or during concurrent marking, whether the &lt;code&gt;SoftReference&lt;/code&gt; should be treated like a strong reference or a weak reference.&lt;/li&gt; &lt;li&gt;&lt;code&gt;WeakReference&lt;/code&gt; objects get cleared and enqueued when the referent is not strongly reachable.&lt;/li&gt; &lt;li&gt;&lt;code&gt;FinalReference&lt;/code&gt; objects get enqueued when the referent is not strongly reachable.&lt;/li&gt; &lt;li&gt;&lt;code&gt;PhantomReference&lt;/code&gt;s get cleared and enqueued when the referent is not strongly reachable and not reachable from any &lt;code&gt;FinalReference&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In other words, our two relevant reachability levels are &lt;em&gt;strongly reachable&lt;/em&gt; and &lt;em&gt;finalizably reachable&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;The traditional implementation determines reachability by marking in the following steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;During concurrent marking, we establish the set of all strongly reachable objects.&lt;/li&gt; &lt;li&gt;We process all references—soft, weak, and final—that require only this reachability level.&lt;/li&gt; &lt;li&gt;We continue marking from finalizees.&lt;/li&gt; &lt;li&gt;We process the remaining phantom references that also require this new information.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Can we determine both strong and finalizable reachability concurrently? Sure we can. But we need to extend our marking bitmap a little bit: Instead of using one bit per object (marked versus unmarked), we now need two bits to represent all possible states: strongly reachable, finalizably reachable, and unreachable (and a 4th state that is used internally). Equipped with this information, we can now concurrently mark through all the live objects and determine both strong and finalizable reachability:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Start marking from roots with normal strong wavefront.&lt;/li&gt; &lt;li&gt;As soon as a strong wavefront encounters a &lt;code&gt;SoftReference&lt;/code&gt;, decide (based on heuristics) whether it should be treated as a strong or weak reference. If the soft references should be treated as strong, normally mark through the referent (marking its subgraph as strong), otherwise stop the wavefront there and enqueue the soft reference in the discovered queue.&lt;/li&gt; &lt;li&gt;When a strong wavefront encounters a &lt;code&gt;WeakReference&lt;/code&gt; or &lt;code&gt;PhantomReference&lt;/code&gt;, stop the wavefront there and enqueue the reference in the discovered queue.&lt;/li&gt; &lt;li&gt;As soon as a strong wavefront encounters a &lt;code&gt;FinalReference&lt;/code&gt;, mark that &lt;code&gt;FinalReference&lt;/code&gt; as strong, and switch to the finalizable wavefront for marking the referent. All objects reachable from there will now be marked finalizable.&lt;/li&gt; &lt;li&gt;When a strong wavefront encounters an object that is already marked finalizable, upgrade the object and its subgraph to strong.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;We now have concurrently marked all reachable objects and determined whether they are strongly or finalizably reachable. We also have enqueued all reference objects in our discovered queue for further processing. This solves the first half of the problem. We still need to clear the unreachable referents and enqueue the reference objects.&lt;/p&gt; &lt;h3&gt;Concurrent reference processing&lt;/h3&gt; &lt;p&gt;We established reachability concurrently during marking. This is finished in the final-mark pause, and we also set up for evacuation during that pause. Concurrent reference processing happens at the beginning of the concurrent evacuation phase. We need to do two things:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Scan the discovered queue and clear all unreachable referents.&lt;/li&gt; &lt;li&gt;Enqueue "unreachable" reference objects in a processing queue for further processing on the Java side.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The tasks are mostly as simple as they sound: We concurrently scan the discovered queue, inspect each reference object, and see whether the referent is reachable. We follow the reachability rules cited early: to be reachable, soft, weak, and final references must be strongly reachable, while phantom references must be either strongly or finalizably reachable. If a reference is unreachable, clear the referent and put the reference into the processing queue.&lt;/p&gt; &lt;p&gt;But wait: What if the Java program tries to access the referent before we cleared it? It would still see the referent that we determined to be otherwise unreachable, and would thus resurrect it. This would be a violation of the spec and cause all sorts of troubles, up to JVM crashes, because the GC would subsequently reclaim or override that object, and the Java program would end up with a dangling pointer.&lt;/p&gt; &lt;p&gt;The solution to this problem is a special barrier that we insert in &lt;code&gt;Reference.get()&lt;/code&gt;. We already have an LRB there because it is a reference load:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;// Pseudocode of Reference.get() intrinsic T Reference_get() { T ref = this.referent; return lrb(ref); } Let's return null when the referent is unreachable: // Pseudocode of Reference.get() for concurrent reference processing T Reference_get() { T ref = this.referent; if (isUnreachable(ref) { return null; } return lrb(ref); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Voila!&lt;/em&gt; We now always return &lt;code&gt;null&lt;/code&gt; whenever we try to access an unreachable referent, and the Java application never gets to resurrect an object by accident.&lt;/p&gt; &lt;h2&gt;Enough theory, show me the numbers&lt;/h2&gt; &lt;p&gt;How bad is concurrent garbage collection, actually?&lt;/p&gt; &lt;p&gt;Let's look at a workload that makes modest use of references and finalizees, running under JDK 11. The &lt;code&gt;-Xlog:gc+stats&lt;/code&gt; option gives us some statistics:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;Pause Final Mark (N) = 0,278 s (a = 3812 us) (n = 73) (lvls, us = 893, 2891, 3535, 4414, 7707) Finish Queues = 0,020 s (a = 273 us) (n = 73) (lvls, us = 94, 123, 137, 227, 4426) Weak References = 0,214 s (a = 2929 us) (n = 73) (lvls, us = 158, 2109, 2695, 3555, 6072) Process = 0,213 s (a = 2924 us) (n = 73) (lvls, us = 154, 2109, 2695, 3555, 6067)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This shows that out of the average final-mark pause of 3.8ms, GC spends 2.9ms in weak reference processing, and in the worst case even 4.4ms out of 7.7ms.&lt;/p&gt; &lt;p&gt;Let's look at the same code with JDK 16:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;Pause Final Mark (G) = 0,208 s (a = 2044 us) (n = 102) (lvls, us = 688, 820, 914, 1562, 36975) Pause Final Mark (N) = 0,089 s (a = 870 us) (n = 102) (lvls, us = 500, 625, 705, 764, 4489) Finish Queues = 0,034 s (a = 329 us) (n = 102) (lvls, us = 107, 125, 174, 246, 4072) Update Region States = 0,004 s (a = 38 us) (n = 102) (lvls, us = 26, 34, 38, 41, 52) Manage GC/TLABs = 0,001 s (a = 13 us) (n = 102) (lvls, us = 9, 13, 13, 14, 22) Choose Collection Set = 0,019 s (a = 183 us) (n = 102) (lvls, us = 97, 156, 188, 203, 323) Rebuild Free Set = 0,002 s (a = 23 us) (n = 102) (lvls, us = 13, 20, 23, 25, 29) Initial Evacuation = 0,028 s (a = 274 us) (n = 102) (lvls, us = 133, 207, 227, 260, 3264) E: = 0,247 s (a = 2422 us) (n = 102) (lvls, us = 799, 1875, 2031, 2090, 39583) E: Thread Roots = 0,247 s (a = 2422 us) (n = 102) (lvls, us = 799, 1875, 2031, 2090, 39583) Concurrent Weak References = 0,967 s (a = 9479 us) (n = 102) (lvls, us = 152, 4824, 9473, 11914, 39231) Process = 0,966 s (a = 9470 us) (n = 102) (lvls, us = 148, 4805, 9453, 11914, 39215) [&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Reference processing is completely gone from the final-mark pause, and the pause is down to 0.9ms. Reference processing is now listed separately under "Concurrent Weak References" and takes an average of 9.5ms—but we don't care all that much because it doesn't keep the application from running.&lt;/p&gt; &lt;p&gt;These numbers are from a relatively modest workload. We have worked with a customer workload with literally millions of weak references and finalizees, and as is to be expected, the effect is much more dramatic there.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;If you are using weak references, soft reference caches, finalizees, or phantom references for resource cleanups, and you care about garbage collection pauses and latency, you might want to upgrade to JDK 16 and give Shenandoah GC a try.&lt;/p&gt; &lt;p&gt;For more information about Java, please refer to the related &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Red Hat Developer topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/05/20/shenandoah-garbage-collection-openjdk-16-concurrent-reference-processing" title="Shenandoah garbage collection in OpenJDK 16: Concurrent reference processing"&gt;Shenandoah garbage collection in OpenJDK 16: Concurrent reference processing&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/kLbotXu8hOI" height="1" width="1" alt=""/&gt;</summary><dc:creator>Roman Kennke</dc:creator><dc:date>2021-05-20T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/05/20/shenandoah-garbage-collection-openjdk-16-concurrent-reference-processing</feedburner:origLink></entry><entry><title type="html">Rule Impact Analysis</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/NS_gwb3qtmg/rule-impact-analysis.html" /><author><name>Toshiya Kobayashi</name></author><id>http://feeds.athico.com/~r/droolsatom/~3/D8Zmjp4sa6M/rule-impact-analysis.html</id><updated>2021-05-20T06:45:00Z</updated><content type="html">After you develop rules and put the system into production, you will need to maintain the rules to keep up with business requirements. Basically tests should ensure the correctness and integrity of the updated rules, but while you work on updating the rules, you might want to know the "impact" of your changes. Rule impact analysis feature helps you. drools-impact-analysis is a new experimental module to analyze impacts of changes in rules, which is available since drools 7.54.0.Final. drools-impact-analysis parses rules and visualizes the relationships between the rules. Also it can render impacted rules when you specify a rule which you plan to change. Let’s see how to use this feature. USING THE IMPACT ANALYSIS FEATURE Example code can be found in * Have drools-impact-analysis-graph-graphviz in your project dependency &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-impact-analysis-graph-graphviz&lt;/artifactId&gt; &lt;version&gt;${drools.version}&lt;/version&gt; &lt;/dependency&gt; * Create a KieFileSystem to store your assets. Then call KieBuilder.buildAll(ImpactAnalysisProject.class). You can get AnalysisModel. // set up KieFileSystem ... KieBuilder kieBuilder = KieServices.Factory.get().newKieBuilder(kfs).buildAll(ImpactAnalysisProject.class); ImpactAnalysisKieModule analysisKieModule = (ImpactAnalysisKieModule) kieBuilder.getKieModule(); AnalysisModel analysisModel = analysisKieModule.getAnalysisModel(); * Convert the AnalysisModel to Graph using ModelToGraphConverter ModelToGraphConverter converter = new ModelToGraphConverter(); Graph graph = converter.toGraph(analysisModel); * Specify a rule which you plan to change. ImpactAnalysisHelper will produce a sub graph which contains the changed rule and impacted rules ImpactAnalysisHelper impactFilter = new ImpactAnalysisHelper(); Graph impactedSubGraph = impactFilter.filterImpactedNodes(graph, "org.drools.impact.analysis.example.PriceCheck_11"); * Generate a graph image using GraphImageGenerator. You can choose the format from DOT, SVG and PNG. GraphImageGenerator generator = new GraphImageGenerator("example-impacted-sub-graph"); generator.generateSvg(impactedSubGraph); * Simple text output is also available using TextReporter. You can choose the format from HierarchyText and FlatText. String hierarchyText = TextReporter.toHierarchyText(impactedSubGraph); System.out.println(hierarchyText); TIPS A typical use case is to view an impacted sub graph because a whole graph could be too large if you have many rules. Red node is a changed rule. Yellow nodes are impacted rules. Solid arrow represents positive impact, where the source rule activates the target rule. Dashed arrow represents negative impact, where the source rule deactivates the target rule. Dotted arrow represents unknown impact, where the source rule may activate or deactivate the target rule. You can collapse the graph based on rule name prefix (= RuleSet in spreadsheet) using GraphCollapsionHelper. It will help you to see the overview. You can also use ImpactAnalysisHelper to the collapsed graph. Graph collapsedGraph = new GraphCollapsionHelper().collapseWithRuleNamePrefix(graph); Graph impactedCollapsedSubGraph = impactFilter.filterImpactedNodes(collapsedGraph, "org.drools.impact.analysis.example.PriceCheck"); You can filter the relations by giving positiveOnly to true for ModelToGraphConverter, ImpactAnalysisHelper and GraphCollapsionHelper constructor. So you can view only positive relations. ModelToGraphConverter converter = new ModelToGraphConverter(true); Graph graph = converter.toGraph(analysisModel); ImpactAnalysisHelper impactFilter = new ImpactAnalysisHelper(true); Graph impactedSubGraph = impactFilter.filterImpactedNodes(graph, "org.drools.impact.analysis.example.PriceCheck_11"); If the number of rules is very large, text output would be useful. [*] is a changed rule. [+] is impacted rules. A rule with parentheses means a circular reference so it doesn’t render further. --- toHierarchyText --- Inventory shortage[+] PriceCheck_11[*] StatusCheck_12[+] (Inventory shortage) StatusCheck_13[+] StatusCheck_11[+] (PriceCheck_11) --- toFlatText --- Inventory shortage[+] PriceCheck_11[*] StatusCheck_11[+] StatusCheck_12[+] StatusCheck_13[+] Note: drools-impact-analysis can parse DRLs and related assets (e.g. spreadsheets). DMN is not subject to this feature but you know that DMN already renders relationships between decisions using DRD. Rule impact analysis is a brand new experimental feature so there is much room to improve (e.g. usability). We really appreciate your feedback. Thanks! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/NS_gwb3qtmg" height="1" width="1" alt=""/&gt;</content><dc:creator>Toshiya Kobayashi</dc:creator><feedburner:origLink>http://feeds.athico.com/~r/droolsatom/~3/D8Zmjp4sa6M/rule-impact-analysis.html</feedburner:origLink></entry><entry><title>Authorizing multi-language microservices with oauth2-proxy</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/hrU1e-JIgnY/authorizing-multi-language-microservices-oauth2-proxy" /><author><name>Rarm Nagalingam</name></author><id>8205e5c8-5bf0-42ab-a3e6-f650632c3fd5</id><updated>2021-05-20T03:00:00Z</updated><published>2021-05-20T03:00:00Z</published><summary type="html">&lt;p&gt;In an article published in August 2020, &lt;a href="https://developers.redhat.com/blog/2020/08/03/authorizing-multi-language-microservices-with-louketo-proxy/"&gt;Authorizing multi-language microservices with Louketo Proxy&lt;/a&gt;, I explained how to use Louketo Proxy to provide authentication and authorization to your &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;. Since then, the Louketo Proxy project has reached its end of life, with developers recommending the &lt;a href="https://oauth2-proxy.github.io/oauth2-proxy/"&gt;oauth2-proxy&lt;/a&gt; project as an alternative.&lt;/p&gt; &lt;p&gt;In this article, I will outline how to secure a microservice with &lt;a href="https://www.keycloak.org/"&gt;Keycloak&lt;/a&gt; and oauth2-proxy.&lt;/p&gt; &lt;h2&gt;Using Keycloak&lt;/h2&gt; &lt;p&gt;The following sections describe how to set up Keycloak on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; for the scenario in this article.&lt;/p&gt; &lt;h3&gt;Test and deploy the Keycloak server&lt;/h3&gt; &lt;p&gt;Begin by testing the Keycloak server. Use the following commands to deploy the Keycloak server on OpenShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export PROJECT="keyauth" $ oc new-project ${PROJECT} $ oc new-app --name sso \ --docker-image=quay.io/keycloak/keycloak \ -e KEYCLOAK_USER='admin' \ -e KEYCLOAK_PASSWORD='oauth2-demo' \ -e PROXY_ADDRESS_FORWARDING='true' \ -n ${PROJECT} $ oc create route edge --service=sso -n ${PROJECT}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We now need to add a client configuration to Keycloak so it can secure our application.&lt;/p&gt; &lt;h3&gt;Create and configure the oauth2 authentication client&lt;/h3&gt; &lt;p&gt;Log in to Keycloak with the username &lt;code&gt;admin&lt;/code&gt; and password &lt;code&gt;oauth2-demo&lt;/code&gt;. On the Keycloak user interface (UI), select &lt;strong&gt;Clients&lt;/strong&gt; on the left navigation bar and select &lt;strong&gt;Create&lt;/strong&gt;. On the &lt;strong&gt;Add Client&lt;/strong&gt; page, fill out all the necessary fields, and click &lt;strong&gt;Save&lt;/strong&gt; to create a new client, as shown in Figure 1.&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/01_create_client.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/01_create_client.png?itok=_GRhUbWk" width="600" height="292" title="01_create_client" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 1: Add the required field values on the Add Client page on the Keycloak UI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After adding the new client, go to the &lt;strong&gt;Oauth2-proxy&lt;/strong&gt; page, and switch the client's &lt;strong&gt;Access Type&lt;/strong&gt; field from public to confidential, as shown in Figure 2.&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/02_confidential.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/02_confidential.png?itok=wKcNFscm" width="600" height="437" title="02_confidential" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 2: Switch the Access Type field (client protocol) from public to confidential on the Oauth2-proxy page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Next, set a valid callback URL for our oauth2-proxy securing our application. In this scenario, oauth2-proxy is securing a flask app. The URL is similar to the Keycloak URL, although instead of the prefix &lt;code&gt;sso&lt;/code&gt;, it uses &lt;code&gt;flask&lt;/code&gt;. For example, if your Keycloak URL is:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;https://sso-keyauth.apps-crc.testing&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;you should set the following URL in the &lt;strong&gt;Valid Redirect URIs&lt;/strong&gt; field:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;https://flask-keyauth.apps-crc.testing/oauth2/callback&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 3 shows the dialog to create the redirect URL.&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/03_callback_url.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/03_callback_url.png?itok=FR8iztq5" width="560" height="205" title="03_callback_url" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 3: Set a valid redirect callback URL using flask and take note of the generated secret.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Then click &lt;strong&gt;Save&lt;/strong&gt; on the client protocol of confidential, the page displays a new tab, &lt;strong&gt;Credentials&lt;/strong&gt;. Click the tab and take note of the generated secret&lt;/p&gt; &lt;h3&gt;Configure the mappers&lt;/h3&gt; &lt;p&gt;Keycloak supports passing group memberships of users to the microservice as &lt;strong&gt;X-Forwarded-Groups&lt;/strong&gt;. This can be enabled by configuring a group mapper. This is a useful way to expose authorization functions within the microservice. For example we could restrict privileged functions of the microservices to users in the admin group.&lt;/p&gt; &lt;p&gt;Select the &lt;strong&gt;Mappers&lt;/strong&gt; tab on the &lt;strong&gt;Create Protocol Mapper&lt;/strong&gt; page, add a new mapper and enter all the groups using the following settings:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt;: &lt;code&gt;groups&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mapper Type&lt;/strong&gt;: Group Membership&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Token Claim Name&lt;/strong&gt;: &lt;code&gt;groups&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full group path&lt;/strong&gt;: OFF&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Click &lt;strong&gt;Save&lt;/strong&gt; when the fields are complete. Figure 4 shows the &lt;strong&gt;Create Protocol Mapper&lt;/strong&gt; page with all fields complete.&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/04_groups_mapper.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/04_groups_mapper.png?itok=UVl1MG8K" width="413" height="420" title="04_groups_mapper" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 4: Create a new mapper with all the necessary field values to the microservice for X-Forwarded-Groups.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Configure the user groups&lt;/h3&gt; &lt;p&gt;Now select &lt;strong&gt;Groups&lt;/strong&gt; from the left navigation bar and add two groups:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;admin&lt;/li&gt; &lt;li&gt;basic_user&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Configure the user&lt;/h3&gt; &lt;p&gt;From the left navigation bar, select &lt;strong&gt;Users&lt;/strong&gt; and add a user. Enter an email address and a password for the new user, and add the user to the &lt;strong&gt;basic_user&lt;/strong&gt; and &lt;strong&gt;admin&lt;/strong&gt; groups you've just created. Now you are ready to configure oauth2-proxy and the example application.&lt;/p&gt; &lt;h2&gt;Deploy the application with oauth2-proxy sidecar&lt;/h2&gt; &lt;p&gt;Now let's deploy our application with an oauth2-proxy sidecar. To make life easier, here is a Git repository with all of the OpenShift templates. Clone the repository and &lt;code&gt;cd&lt;/code&gt; into the folder and run the following script to configure and deploy all of the templates, simply passing the OpenShift project name as a &lt;code&gt;variable&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/snowjet/demo-oauth2-proxy.git $ cd demo-oauth2-proxy # deploy flask with oauth2-proxy in project keyauth $ ./create_app.sh keyauth&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The script above deploys a number of templates for the application. However, the important templates relating to oauth2-proxy are the configuration maps.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;Configmap_ssopubkey.yml&lt;/code&gt; contains the Keycloak Public Certificate to validate the JSON Web Token (JWT) passed by oauth2-proxy&lt;/li&gt; &lt;li&gt;&lt;code&gt;Configmap-oauth.yml&lt;/code&gt; contains the variables required to configure oauth2-proxy. These include: &lt;ul&gt;&lt;li&gt;&lt;code&gt;oidc_issuer_url&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;client_secret&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;redirect_url&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;cookie_secret&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;whitelist_domains&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;cookie_domains&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The remaining templates complete the deployment:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;DeploymentConfig&lt;/code&gt; for the application with a sidecar for oauth2-proxy&lt;/li&gt; &lt;li&gt;Service pointing to the oauth2-proxy&lt;/li&gt; &lt;li&gt;Route pointing to the service for the oauth2-proxy&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Testing the configuration&lt;/h2&gt; &lt;p&gt;Once deployed, test the configuration and then browse to the example application. Remember to sign out of the admin portion of Keycloak before trying to sign in to the Keycloak web app. Then, on the &lt;strong&gt;Sign in&lt;/strong&gt; screen, click &lt;strong&gt;Sign in with Keycloak&lt;/strong&gt;, as shown in Figure 5.&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/05_sign_in.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/05_sign_in.png?itok=vi7u-8cC" width="487" height="148" title="05_sign_in" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 5: Sign in to test the configuration and browse to the example application.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once you are redirected to the Keycloak login dialog, type the username or email address and the password for the application user, as shown in Figure 6. Then click the login button to complete your entries.&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/06_login_sso.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/06_login_sso.png?itok=ErbDGHkV" width="600" height="494" title="06_login_sso" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 6: On the Keycloak login dialog, type the username or email address and password for the application user.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After you are successfully authenticated, you will be redirected to an application page that returns a JSON file like the one shown in Figure 7. The file exposes the headers passed along by oauth2-proxy:&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/07_json.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/07_json.png?itok=fY4zh11f" width="600" height="457" title="07_json" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 7: Redirects to an application page that returns a JSON file.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article was an introduction to oauth2-proxy, a replacement for Louketo Proxy that provides authentication for your applications without making you code OpenID Connect clients within microservices. As always, I welcome your questions and any feedback and details you share in the comments.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/05/20/authorizing-multi-language-microservices-oauth2-proxy" title="Authorizing multi-language microservices with oauth2-proxy"&gt;Authorizing multi-language microservices with oauth2-proxy&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/hrU1e-JIgnY" height="1" width="1" alt=""/&gt;</summary><dc:creator>Rarm Nagalingam</dc:creator><dc:date>2021-05-20T03:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/05/20/authorizing-multi-language-microservices-oauth2-proxy</feedburner:origLink></entry><entry><title>Red Hat Enterprise Linux 8.4 now generally available</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/KURwpOeOG7A/red-hat-enterprise-linux-84-now-generally-available" /><author><name>Don Pinto</name></author><id>4096faef-e827-4fa2-be19-c29377974e32</id><updated>2021-05-19T13:00:00Z</updated><published>2021-05-19T13:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux 8.4&lt;/a&gt;, which was pre-announced on April 27 at &lt;a href="https://www.redhat.com/en/summit"&gt;Red Hat Summit&lt;/a&gt;, is now generally available. We encourage &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; developers to download this latest release and try out the new software. We also recommend updating both development, and production systems to the new &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux &lt;/a&gt;(RHEL) 8.4 release.&lt;/p&gt; &lt;h2&gt;What's new in RHEL 8.4?&lt;/h2&gt; &lt;p&gt;RHEL 8.4 delivers a streamlined path from development to deployment that unifies teams across a single open platform, including the tools and analytics needed to build and manage these systems on any footprint—from the data center to the cloud to &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;the edge&lt;/a&gt;, and beyond. With access to the latest tools, programming languages, and enhanced container capabilities, development teams can achieve faster time to value when producing new code. You can learn more about what RHEL 8.4 provides &lt;a href="https://www.redhat.com/en/blog/rhel-84-brings-continuous-stability-plus-innovation"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you’re a developer, here are some key highlights that you need to know about Red Hat Enterprise Linux 8.4.&lt;/p&gt; &lt;h3&gt;Leverage the latest database technology&lt;/h3&gt; &lt;p&gt;Now, you can leverage the latest database technology in your applications:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;PostgreSQL 13&lt;/strong&gt;, now available through RHEL application streams, improves database performance and lets developers modernize their applications, especially in the cloud.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Redis 6&lt;/strong&gt;, now available in RHEL application streams, allows developers to build modern apps that can leverage the new database security enhancements, and client-side caching features to boost performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MariaDB 10.5&lt;/strong&gt;, now available in RHEL application streams, lets you build apps that can leverage added database features, including IPv6 (INET 6) data type support, more granular privileges, and clustering with the Galera plugin.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Power your applications with the latest application runtimes&lt;/h3&gt; &lt;p&gt;Application runtime updates bring new features to your applications:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Python 3.9&lt;/strong&gt; brings several new enhancements including timezone-aware timestamps, the new string prefix, and suffix methods, and dictionary union operations, so that developers can modernize their apps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Go 1.15&lt;/strong&gt; brings improved memory allocation for small objects, improvements to the Go linker, and several other core library improvements.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rust 1.49&lt;/strong&gt; allows developers to write high-performance applications that run with a low memory footprint making it highly suitable for edge use cases. Additionally, Rust is a statically typed language making it easy to catch errors at compile-time and maintain.&lt;/li&gt; &lt;li&gt;With the latest &lt;strong&gt;LLVM toolset&lt;/strong&gt;, developers can take advantage of fresher tooling, and compatibility with other code built with compatible versions of &lt;a href="https://developers.redhat.com/search?t=clang+llvm"&gt;LLVM/Clang&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Build, share, and collaborate on RHEL applications with your friends and customers&lt;/h3&gt; &lt;p&gt;Container updates make it easier to build, share, and collaborate on RHEL applications:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Red Hat Universal Base Image (&lt;/strong&gt;&lt;a href="https://www.redhat.com/en/blog/introducing-red-hat-universal-base-image"&gt;&lt;strong&gt;UBI&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;): &lt;/strong&gt;The certified language runtime containers have been updated and include containerized environments for some of the languages listed above. Additionally, with RHEL 8.4, a new, micro UBI container offering is available in the catalog to provide an even smaller than minimal base container image.&lt;/li&gt; &lt;li&gt;Looking for a specific container image? Check out the &lt;a href="https://connect.redhat.com/explore/red-hat-container-certification"&gt;&lt;strong&gt;Red Hat Certified Containers&lt;/strong&gt;&lt;/a&gt; through the &lt;a href="https://catalog.redhat.com/software/containers/explore"&gt; Red Hat Ecosystem Catalog&lt;/a&gt;. This makes it easier to build and deploy mission-critical applications using the supported application streams for Red Hat Enterprise Linux and &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt; Red Hat OpenShift &lt;/a&gt; environments.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Get started with RHEL 8.4 today&lt;/h3&gt; &lt;p&gt;Red Hat Enterprise Linux 8.4 continues Red Hat’s commitment to customer choice in terms of the underlying compute architecture, with availability across x86_64, ppc64le, s390x, and aarch64 hardware.&lt;/p&gt; &lt;p&gt;Developers with active subscriptions can access &lt;a href="https://access.redhat.com/downloads/content/479/ver=/rhel---8/8.4/x86_64/product-software"&gt;Red Hat Enterprise Linux downloads&lt;/a&gt;. If you’re new to using Red Hat products, register for the Red Hat developer program to get access to the &lt;a href="https://developers.redhat.com/rhel8"&gt;&lt;strong&gt;Individual Developer subscription&lt;/strong&gt;&lt;/a&gt; for RHEL, which can be used in production for up to 16 systems. For information about a &lt;a href="https://www.redhat.com/en/blog/new-year-new-red-hat-enterprise-linux-programs-easier-ways-access-rhel#Bookmark%202"&gt;&lt;strong&gt;Red Hat Developer for Teams subscription&lt;/strong&gt;&lt;/a&gt;, contact your Red Hat account representative.&lt;/p&gt; &lt;p&gt;For more information, please read the full &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/"&gt;release notes.&lt;/a&gt;&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/05/19/red-hat-enterprise-linux-84-now-generally-available" title="Red Hat Enterprise Linux 8.4 now generally available"&gt;Red Hat Enterprise Linux 8.4 now generally available&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/KURwpOeOG7A" height="1" width="1" alt=""/&gt;</summary><dc:creator>Don Pinto</dc:creator><dc:date>2021-05-19T13:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/05/19/red-hat-enterprise-linux-84-now-generally-available</feedburner:origLink></entry><entry><title>micropipenv: Installing Python dependencies in containerized applications</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/JW_9dWQdZIE/micropipenv-installing-python-dependencies-containerized-applications" /><author><name>Fridolin Pokorny, Lumír Balhar</name></author><id>0c2b09c1-86ab-496d-afa1-48aaac77e981</id><updated>2021-05-19T03:00:00Z</updated><published>2021-05-19T03:00:00Z</published><summary type="html">&lt;p&gt;Trends in the software engineering industry show that the &lt;a href="https://insights.stackoverflow.com/survey/2020#technology-programming-scripting-and-markup-languages"&gt;Python programming language is growing in popularity&lt;/a&gt;. Properly managing &lt;a href="https://developers.redhat.com/blog/category/python/"&gt;Python&lt;/a&gt; dependencies is crucial to guaranteeing a healthy software development life cycle. In this article, we will look at installing Python dependencies for Python applications into &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized environments&lt;/a&gt;, which also have become very popular. In particular, we introduce &lt;a href="https://pypi.org/project/micropipenv"&gt;micropipenv&lt;/a&gt;, a tool we created as a compatibility layer on top of pip (the Python package installer) and related installation tools. The approach discussed in this article ensures that your applications are shipped with the desired software for the purposes of traceability or integrity. The approach provides reproducible Python applications across different application builds done over time.&lt;/p&gt; &lt;h2&gt;Python dependency management&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/open-source"&gt;Open source&lt;/a&gt; community efforts provide tools to manage application dependencies. The most popular such tools for Python are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://pypi.org/project/pip/"&gt;pip&lt;/a&gt; (offered by the Python Packaging Authority)&lt;/li&gt; &lt;li&gt;&lt;a href="https://pypi.org/project/pip-tools/"&gt;pip-tools&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pypi.org/project/pipenv/"&gt;Pipenv&lt;/a&gt; (offered by the Python Packaging Authority)&lt;/li&gt; &lt;li&gt;&lt;a href="https://pypi.org/project/poetry/"&gt;Poetry&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Each of these tools has its own pros and cons, so developers can choose the right tooling based on their preferences.&lt;/p&gt; &lt;h3&gt;Virtual environment management&lt;/h3&gt; &lt;p&gt;One of the features that can be important for developers is implicit virtual environment management, offered by Pipenv and Poetry. This feature is a time-saver when developing applications locally, but can have drawbacks when installing and providing the application in a container image. One of the drawbacks of adding this layer is its potentially &lt;a href="https://github.com/fridex/s2i-example-micropipenv#micropipenv-vs-pipenv-size"&gt;negative impact on container-image size&lt;/a&gt; because the tools add bulk to the software within the container image.&lt;/p&gt; &lt;p&gt;On the other hand, pip and pip-tools require explicit virtual environment management when developing applications locally. With explicit virtual environment management, the application dependencies do not interfere with system Python libraries or other dependencies shared across multiple projects.&lt;/p&gt; &lt;h3&gt;The lock file&lt;/h3&gt; &lt;p&gt;Even though pip is the most fundamental tool for installing Python dependencies, it does not provide an implicit mechanism for managing the whole dependency graph. This gave the pip-tools developers an opportunity to design pip-tools to manage a locked-down dependency listing, featuring direct dependencies and transitive dependencies based on application requirements.&lt;/p&gt; &lt;p&gt;Stating all the dependencies in the lock file provides fine-grained control over which Python dependencies in which versions are installed at any point in time. If developers do not lock down all the dependencies, they might confront issues that can arise over time due to new Python package releases, &lt;a href="https://www.python.org/dev/peps/pep-0592/"&gt;yanking specific Python releases (PEP-592)&lt;/a&gt;, or the complete removal of Python packages from Python package indexes such as &lt;a href="https://pypi.org/"&gt;PyPI&lt;/a&gt;. All of these actions can introduce undesired and unpredictable issues created by changes across releases in the dependencies installed. Maintaining and shipping the lock file with the application avoids such issues and provides traceability to application maintainers and developers.&lt;/p&gt; &lt;h3&gt;Digests of installed artifacts&lt;/h3&gt; &lt;p&gt;Even though pip-tools states all the dependencies that are installed in specific versions in its lock file, we recommend including digests of installed artifacts by providing the &lt;a href="https://pypi.org/project/pip-tools/"&gt;--generate-hashes option to the pip-compile command&lt;/a&gt;, because this is not done by default. The option triggers integrity checks of installed artifacts during the installation process. Digests of installed artifacts are automatically included in lock files managed by Pipenv or Poetry.&lt;/p&gt; &lt;p&gt;On the other hand, pip cannot generate hashes of packages that are already installed. However, pip performs checks during the installation process when digests of artifacts are provided explicitly or when you supply the &lt;a href="https://pip.pypa.io/en/stable/cli/pip_install/"&gt;--require-hashes option&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To support all the tools discussed in this section, we introduced micropipenv. The rest of this article  explains how it works and how it fits into the Python environment.&lt;/p&gt; &lt;h2&gt;Installing Python dependencies with micropipenv&lt;/h2&gt; &lt;p&gt;micropipenv parses the requirements or lock files produced by the tools discussed in the previous section: &lt;code&gt;requirements.txt&lt;/code&gt;, &lt;code&gt;Pipfile&lt;/code&gt;/&lt;code&gt;Pipfile.lock&lt;/code&gt;, and &lt;code&gt;pyproject.toml&lt;/code&gt;/&lt;code&gt;poetry.lock&lt;/code&gt;. micropipenv tightly cooperates with the other tools and acts as a small addition to pip that can prepare dependency installation while respecting requirements files and lock files. All the main benefits of the core pip installation process stay untouched.&lt;/p&gt; &lt;p&gt;By supporting all the files produced by pip, pip-tools, Pipenv, and Poetry, micropipenv allows users to employ the tool of their choice for installing and managing Python dependencies in their projects. Once the application is ready to be shipped within the container image, developers can seamlessly use all recent &lt;a href="https://github.com/sclorg/s2i-python-container"&gt;Python Source-To-Image&lt;/a&gt; (S2I) container images based on Python 3. These images offer micropipenv functionality. You can subsequently use the applications in deployments managed by &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Figure 1 shows micropipenv as a common layer for installing Python dependencies in an OpenShift deployment.&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/01/Screenshot_2021-01-07_14-56-15.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/01/Screenshot_2021-01-07_14-56-15.png?itok=bu2OPWT-" width="600" height="258" title="micropipenv in OpenShift s2i" typeof="Image" /&gt;&lt;/a&gt; &lt;figcaption class="rhd-media-caption field__item"&gt; micropipenv serving common layer in OpenShift Python S2I. &lt;/figcaption&gt;&lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 1: micropipenv in an OpenShift Python S2I deployment.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To enable micropipenv in the Python &lt;a href="https://docs.openshift.com/container-platform/4.6/builds/build-strategies.html#builds-strategy-s2i-build_build-strategies"&gt;S2I build process&lt;/a&gt;, export the &lt;code&gt;ENABLE_MICROPIPENV=1&lt;/code&gt; environment variable. See the &lt;a href="https://github.com/sclorg/s2i-python-container/tree/master/3.6#environment-variables"&gt;documentation for more details&lt;/a&gt;. This feature is available in all recent &lt;a href="https://github.com/sclorg/s2i-python-container/"&gt;S2I container images&lt;/a&gt; based on Python 3 and built on top of Fedora, CentOS Linux, &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Images&lt;/a&gt; (UBI), or &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL). Even though micropipenv was originally designed for containerized Python S2I container images, we believe that &lt;a href="https://github.com/thoth-station/micropipenv/blob/deed020fb3ee57ab1aa7df8a46419393b501d7f3/README.rst#micropipenv-use-cases"&gt;it will find use cases elsewhere&lt;/a&gt;, such as when installing dependencies without lock file management tools or when converting between lock files of different types. We also found the tool &lt;a href="https://github.com/thoth-station/jupyterlab-requirements/"&gt;suitable for assisting with dependency installation in Jupyter Notebook&lt;/a&gt; to &lt;a href="https://developers.redhat.com/blog/2021/03/19/managing-python-dependencies-with-the-thoth-jupyterlab-extension/"&gt;support reproducible data science environments&lt;/a&gt;.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Please check the &lt;a href="https://www.youtube.com/watch?v=I-QC83BcLuo&amp;t=8m58s"&gt;Project Thoth scrum demo&lt;/a&gt; if you are interested in Thoth's Python S2I integration (the micropipenv demo starts at 9:00). You can also check the &lt;a href="https://www.youtube.com/watch?v=QT4ConYPSOA"&gt;Improvements in OpenShift S2I&lt;/a&gt; talk presented at the &lt;a href="https://developers.redhat.com/devnation"&gt;DevNation 2019&lt;/a&gt; conference. &lt;a href="https://github.com/thoth-station/talks/blob/master/2020-09-25-devconf-us/thoth_python_s2i.pdf"&gt;Slides&lt;/a&gt; and a &lt;a href="https://devconfus2020.sched.com/event/dkXi/improvements-in-openshift-python-s2i"&gt;description of the talk&lt;/a&gt; are also available online.&lt;/p&gt; &lt;h2&gt;Benefits of micropipenv&lt;/h2&gt; &lt;p&gt;We wanted to bring Pipenv or Poetry to the Python S2I build process because of their advantages for developers. But from the perspective of RPM package maintenance, both Pipenv and Poetry are hard to package and maintain in the standard way we use RPM in Fedora, CentOS, and RHEL. Pipenv bundles all of its more than 50 dependencies, and the list of these dependencies changes constantly. Package tools like these are complex, so maintaining them and fixing all possible security issues in their bundled dependencies could be very time-consuming.&lt;/p&gt; &lt;p&gt;Moreover, micropipenv brings unified installation logs. Logs are not differentiated based on the tool used, and provide insights into issues that can arise during installation.&lt;/p&gt; &lt;p&gt;Another reason for micropipenv is that a Pipenv installation uses more than 18MB of disk space, which is a lot for a tool we need to use only once during the container build.&lt;/p&gt; &lt;p&gt;We have already packaged and prepared &lt;a href="https://src.fedoraproject.org/rpms/micropipenv"&gt;micropipenv as an RPM package&lt;/a&gt;. It is also &lt;a href="https://pypi.org/project/micropipenv"&gt;available on PyPI&lt;/a&gt;; the project is open source and developed on GitHub in the &lt;a href="https://github.com/thoth-station/micropipenv/"&gt;thoth-station/micropipenv repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;At its core, micropipenv depends only on pip. Other features are available when TOML Python libraries are installed (&lt;a href="https://pypi.org/project/toml"&gt;toml&lt;/a&gt; or legacy &lt;a href="https://pypi.org/project/pytoml"&gt;pytoml&lt;/a&gt; are optional dependencies). The minimal dependencies give micropipenv a very light feel, overall. Having just &lt;a href="https://github.com/thoth-station/micropipenv/blob/master/micropipenv.py"&gt;one file in the codebase&lt;/a&gt; helps with project maintenance, in contrast to the much larger Pipenv or Poetry codebases. All the installation procedures are reused from supported &lt;a href="https://pypi.org/project/pip"&gt;pip releases&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Using and developing micropipenv&lt;/h2&gt; &lt;p&gt;You can get micropipenv in any of the following ways:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Providing &lt;a href="https://github.com/sclorg/s2i-python-container/tree/master/3.6#environment-variables"&gt;ENABLE_MICROPIPENV=1 to the Source-To-Image&lt;/a&gt; container build process&lt;/li&gt; &lt;li&gt;&lt;a href="https://src.fedoraproject.org/rpms/micropipenv"&gt;Installing a micropipenv RPM&lt;/a&gt; by running: &lt;pre&gt; $ dnf install micropipenv&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Installing a &lt;a href="https://pypi.org/project/micropipenv"&gt;micropipenv Python package&lt;/a&gt; by running: &lt;pre&gt; $ pip install micropipenv&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To develop and improve micropipenv or submit feature requests, please visit the &lt;a href="https://github.com/thoth-station/micropipenv/"&gt;thoth-station/micropipenv&lt;/a&gt; repository.&lt;/p&gt; &lt;h2&gt;Acknowledgment&lt;/h2&gt; &lt;p&gt;micropipenv was developed at the &lt;a href="http://github.com/aicoe"&gt;Red Hat Artificial Intelligence Center of Excellence&lt;/a&gt; in &lt;a href="http://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt; and brought to you thanks to cooperation with the Red Hat Python maintenance team.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/05/19/micropipenv-installing-python-dependencies-containerized-applications" title="micropipenv: Installing Python dependencies in containerized applications"&gt;micropipenv: Installing Python dependencies in containerized applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/JW_9dWQdZIE" height="1" width="1" alt=""/&gt;</summary><dc:creator>Fridolin Pokorny, Lumír Balhar</dc:creator><dc:date>2021-05-19T03:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/05/19/micropipenv-installing-python-dependencies-containerized-applications</feedburner:origLink></entry><entry><title type="html">Kogito User Task Process API</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/fkXpf8PS2H4/kogito-user-tasks-process-api.html" /><author><name>Francisco Javier Tirado Sarti</name></author><id>http://feeds.athico.com/~r/droolsatom/~3/EqXdsfWLnrc/kogito-user-tasks-process-api.html</id><updated>2021-05-18T08:54:00Z</updated><content type="html">It can be argued that the greatest technical advances in the history of humankind (agriculture, wheel, steam machine, printing…) have augmented production while reducing human effort. This does not imply that these advances have removed human intervention altogether. On the contrary, they have allowed humans to focus on the most relevant parts of the production process, those places where human genius can make a difference. In agriculture, humans are pivotal in the discovery of new breeds; wheeled vehicles propelled by steam machines derivatives still need human drivers to reach their intended destination and printers will be useless without authors willing to write books. Kogito is not an exception. Although the BPMN ecosystem is all about the pursuit of automation, there is still much needed room for direct human intervention in most business processes. Aware of that circumstance, BPMN contributors defined User Task activity: “A User Task is a typical “workflow” Task where a human performer performs the Task with the assistance of a software application and is scheduled through a task list manager of some sort”. A User Task in Kogito is also known as a Human Task. I will be using both terms indistinguishably. This is the first post of a series where I will be discussing Kogito functionality related with User Task. In this one, I will focus on the process API.  A Human Task might be considered a state machine, with at least one initial and one final state. A Human Task might also be viewed as a function, because given a set of inputs it will produce a set of results.  The life cycle of Human Task typically consists of a sequence of phases, starting with an Active phase, which every created task is initialized to, and finishing with a Complete phase. Between these initial and final states, a Task can go through an arbitrary number of phases, including no phase at all between Active and Complete. The fact that there is an arbitrary number of phases is an important difference from jBPM, where all possible transitions were predefined and immutable, while in Kogito it is up to the final user to decide which transitions are valid, by creating its own custom phases. A User Task can also be interpreted as a human function, where a human, after looking at the input parameters, might add additional information in the form of output parameters or results.  The set of key value pairs that might act as input to the task or be a result of the task completion is known as the Task Model. In Kogito, the Task Model is predefined and generated as part of the compilation of the  process definition. One consequence of what was stated in the previous paragraph is that a human cannot  add more information to the task model without changing process definition. The exception to that rule are comments and attachments:  * A comment consists of a human readable text that will help to achieve a successful resolution of the task.  * An attachment is a reference to an external URIs containing information relevant to the task, for example a screen snapshot.  Note the way attachments are implemented in Kogito is completely  different from the approach taken in jBPM. In jBPM, attachment content was expected to be , while in Kogito, just the URI reference is saved. When during process execution, a User Task node is reached, relevant process properties are passed as input parameters to the task and  the process is paused till the task is completed through human intervention. Once the task reaches its Complete phase, results produced by that task are mapped to process properties and the process execution resumes. KOGITO RUNTIME TASK API Once a task becomes active, it has to be eventually completed to resume the process that instantiates it. Therefore, users should be able to change the current task phase, update tasks results or perform both operations at the same time. They should  also be able to manage comments and attachments. Kogito provides REST APIs to fulfill all these requirements. Let’s split them in functional groups.  PHASE TRANSITIONS The REST template to transition from the current phase to another one is POST http://&lt;host:port&gt;/&lt;process id&gt;/&lt;process instance&gt;/&lt;task name&gt;/&lt;task instance id&gt;/phase/&lt;phase name&gt; As request body, you might optionally provide a JSON object whose key value pairs will be added to the task results.  I guess there are too many template replacements to do, so let’s analyze them one by one, starting with &lt;phase name&gt; Besides initial (Active) and final (Completed) states, Kogito provide these predefined phases ClaimAllow the user to take ownership of the taskReleaseAllow the owner to free ownership of the taskAbortThe task is aborted and process will finish with failureSkipThe task is skipped and process will resume execution In addition to them, Kogito allows users to define their custom phases, as described . There are two pivotal methods to consider when adding a custom phase: mandatory public boolean canTransition(LifeCyclePhase phase), which determines if the transition is allowed from current to target phase and optional void apply(KogitoWorkItem workitem, Transition&lt;?&gt; transition), which gives the user freedom to modify the task information (workitem) using the information passed as body in the REST invocation (transition). You have a nice example of the powerful capabilities (including defining your own security policies) of apply method in implementation.  The rest of template substitutions depend on the process being used, hence I introduce you probably the simplest BPMN using tasks in the world, the one.  Using this process, the value for process id is approval and for task name is firstLineApproval or secondLineApproval.  process instance id should be obtained from the output of the call that starts the process (see for more details on that).  Once the process is started, a task instance for firstLineApproval will be created and the process suspended. To obtain that task instance id, you need to retrieve the list of active tasks: GET http://&lt;host:port&gt;/approvals/&lt;process instance id&gt;/tasks. This API will return a list of Task Model instances. Task Model is a POJO generated from process definition during its compilation. It contains information about the current task phase, tasks input parameters and task output results. Similar to the Task Model, input and output parameters are modelled as generated POJOs, each POJO containing a getter/setter pair per parameter.  In approval process, as you can see in the companion diagrams, both firstLineApproval and secondLineApproval tasks share the same model.  Input data consists of a user defined POJO named . Output is just  a boolean field called approved.  Hence the generated input and output POJOS for firstLineApproval and secondLineApproval are: public class Approvals_1_TaskInput { .... @UserTaskParam(value = ParamType.INPUT) private org.acme.travels.Traveller traveller; public org.acme.travels.Traveller getTraveller() { return traveller; } public void setTraveller(org.acme.travels.Traveller traveller) { this.traveller = traveller; } } public class Approvals_1_TaskOutput implements org.kie.kogito.MapOutput { ..... @UserTaskParam(value = ParamType.OUTPUT) private java.lang.Boolean approved; public java.lang.Boolean getApproved() { return approved; } public void setApproved(java.lang.Boolean approved) { this.approved = approved; } } Therefore, the response of the call performed to retrieve the list of task instances will look this  [ { "id": "07f0f804-030b-4c50-8b61-684db9b748a4", "name": "firstLineApproval", "state": 0, "phase": "active", "phaseStatus": "Ready", "parameters": { "traveller": { "firstName": "John", "lastName": "Doe", "email": "jon.doe@example.com", "nationality": "American", "address": { "street": "main street", "city": "Boston", "zipCode": "10005", "country": "US" } } }, "results": { "approved": null } } ] As expected, the response contains only one active firstlineApproval (field name value). We are particularly interested in the id field, which value is the one to be used as task instance id. Since the task was just created (phaseStatus field is Ready),  there are not any results yet, hence the approved field has a value of null. Besides that, it is worth to mention that the parameters field contains a traveller instance, which was passed as part of the process starting request. Now we have all the information we need to perform the phase transition REST invocation. Assuming you have deployed the process into your host at default port, the call to complete a task for this particular process instance will be (task instance id is highlighted in orange) : URIhttp://localhost:8080/approvals/&lt;process instance id&gt;/firstLineApproval/07f0f804-030b-4c50-8b61-684db9b748a4/phase/completeBody{“approved”:true} Once this call is performed, the task will be completed (not longer being returned by tasks GET  APIs), its output will be set to {“approved”:true} and the process execution will be resumed.   SAVING RESULTS A human task is a potentially long one, so, from time to time it might be wise to save the progress of it, or, as unfortunately happened to me with this post, you might be ending doing it twice.  Saving a human task means adding results to it without performing a phase transition, operation that might be performed by passing as the whole output model as body of this call PUT http://&lt;host:port&gt;/approvals/&lt;process instance id&gt;/&lt;task name&gt;/&lt;task instance id&gt; Note that you need to pass the whole output model (in our approval example that’s not make any difference, since the output model consists of just one parameter) because by convention PUT, as an idempotent method, implies replacing the whole resource.  You can check the model information about an active task at any moment by using GET http://&lt;host:port&gt;/approvals/&lt;process instance id&gt;/&lt;task name&gt;/&lt;task instance id&gt; COMMENTS In Kogito we consider that comments are entities in their own right, so they deserve a dedicated, although not independent, REST resource for them. That way all CRUD operations are supported over them, as illustrated below.  MethodTemplate URIDescriptionGET/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/comments/&lt;comment_id&gt;Retrieve list of comments associated to the taskPUT/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/comments/&lt;comment_id&gt;Updates a comment with the text passed in the bodyPOST/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/commentsCreates a comment with the text passed in the body. Returns the comment idDELETE/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/comments/&lt;comment_id&gt;Deletes the comment ATTACHMENTS Same rationale that was used for comments applies to attachments, the only differences being that the resource name is attachments rather than comments and that the body should be a valid URI, not any random string.  MethodTemplate URIDescriptionGET/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/attachmentsRetrieve list of attachments associated to the taskPUT/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/attachments/&lt;attachment_id&gt;Updates an attachment with the URI passed in the bodyPOST/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/attachmentsCreates an attachment with the URI passed in the body. Returns the attachment idDELETE/&lt;process_id&gt;/&lt;process_instance_id&gt;/&lt;task_name&gt;/&lt;task_instance id&gt;/attachments/&lt;attachment_id&gt;Deletes the attachment CONCLUSION In this post we have gone over the APIs provided by Kogito to operate with tasks from a process perspective. In the next post we will discuss the Task management API, which allows users with enough privileges to change task information not related with the process model, in other words, task fields that are present for any task, regardless process definition. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/fkXpf8PS2H4" height="1" width="1" alt=""/&gt;</content><dc:creator>Francisco Javier Tirado Sarti</dc:creator><feedburner:origLink>http://feeds.athico.com/~r/droolsatom/~3/EqXdsfWLnrc/kogito-user-tasks-process-api.html</feedburner:origLink></entry><entry><title type="html">Real-time stock control - Example stock control architecture</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/SwrPu2KFpSo/real-time-stock-control-example-stock-control-architecture.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/clFgfkzlWDQ/real-time-stock-control-example-stock-control-architecture.html</id><updated>2021-05-18T05:00:00Z</updated><content type="html">Part 3 - Example architecture In our  from this series shared a look at the logical common architectural elements found in a real-time stock control solution for retail organisations. The process was laid out how we've approached the use case and how portfolio solutions are the base for researching a generic architectural blueprint.  It continued by laying out the process of how we've approached the use case by researching successful customer portfolio solutions as the basis for a generic architectural blueprint. Having completed our discussions on the logical view of the blueprint, it's now time to look at a specific example. This article walks you through an example stock control scenario showing how expanding the previously discussed elements provides a blueprint for your own stock control scenarios. BLUEPRINTS REVIEW As mentioned before, the architectural details covered here are base on real solutions using open source technologies. The example scenario presented here is a generic common blueprint that was uncovered researching those solutions. It's our intent to provide a blueprint that provides guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architectural blueprint, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this blueprint and outline the solution for a real-time stock control architecture solution. REAL-TIME STOCK CONTROL The first thing you might notice about this diagram is that there are a lot of external actors involved with any stock control solution. There are customers, associates, vendors, suppliers, and partners all influencing the stock control in some way. For these external actors to the stock control architecture it's crucial to provide consistent authentication and authorisation through the use of API management elements that front all interactions with internal systems. On the left side of this diagram you see that this fronts the collection of available to sell microservices for the customers and store associates to access.  Any changes here are events, so these updates where stock is manipulated are processed through the event streams element and may enact further actions in any of the following elements. These events can trigger retail processes (which also can trigger events as they run) and interactions with any of the external systems through integration microservices. The retail processes element make use of promotions microservices and payments microservices, both elements are related to how stock is being monitored for pricing changes such as when a product is over stocked. To ensure a stock reduction a promotion sale might be triggered. Payments are an integral part of the interactions with customers, vendors, suppliers, and partners that deliver or purchase stock items. The vendors, suppliers, and partners interactions are event generators and shown to be using integration microservices and integration data microservices as the integration point with the Retail Data Framework blueprint, a separate and detailed architecture blueprint to be covered in another series. These interactions also create changes that propagate to the external systems such as catalog management system, logistics system, supply chain system, and order management system. All of these systems are external to the stock control architecture and might be internal systems hosted remotely or completely external systems such as a Software as a Service (SaaS) solution. For this architectural blueprint they have been noted as hosted in a private cloud environment, but that's not a requirement. WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture blueprint for the real-time stock control use case.  An overview of this series on real-time stock control portfolio architecture blueprint: 1. 2. 3. Catch up on any past articles you missed by following any published links above. This completes the series and we hope you enjoyed this architecture blueprint for real-time stock control in retail. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/SwrPu2KFpSo" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/clFgfkzlWDQ/real-time-stock-control-example-stock-control-architecture.html</feedburner:origLink></entry><entry><title>Integrating systems with Apache Camel and Quarkus on Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/UlMs1CUpvjY/integrating-systems-apache-camel-and-quarkus-red-hat-openshift" /><author><name>Peter Palaga</name></author><id>596d68ab-7fdd-484f-a84a-a1068505a500</id><updated>2021-05-18T03:00:00Z</updated><published>2021-05-18T03:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://camel.apache.org/"&gt;Apache Camel&lt;/a&gt; has been a massively successful tool for integrating heterogeneous systems for more than a decade. You have probably dealt with a situation where you have two systems that were not designed to communicate with each other but still need to exchange data. That's exactly the kind of situation where Camel and its integration pipelines can help. This article shows how &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt; uses Camel.&lt;/p&gt; &lt;p&gt;Figure 1 shows Camel's basic operation: Data from one system passes through a transport designed for that system to a transport designed for the recipient system.&lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/05/system1-system2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/05/system1-system2.png?itok=H735pT9_" width="600" height="162" alt="Diagram shows data passing from System A through a System A Transport, and data passing from System B through a System B Transport. The two systems are connected via a Route." typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 1: A basic Camel integration route.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Traditionally, Camel integrations have been deployed on various &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; platforms, such as &lt;a href="https://developers.redhat.com/topics/spring-boot"&gt;Spring Boot&lt;/a&gt;, Karaf, and &lt;a href="https://developers.redhat.com/products/eap/overview"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; (JBoss EAP). Thanks to work done in the &lt;a href="https://camel.apache.org/camel-quarkus/latest/"&gt;Camel Quarkus&lt;/a&gt; community project, it is now possible to run Camel integrations on Quarkus. The main benefit is that integrations start faster and consume less RAM. But Quarkus &lt;code&gt;dev&lt;/code&gt; mode also brings developer-productivity benefits. The &lt;a href="https://developers.redhat.com/topics/containers"&gt;container-first&lt;/a&gt; ethos is also not to be forgotten.&lt;/p&gt; &lt;p&gt;This article shows how to use Camel with Quarkus on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Defining Camel routes&lt;/h2&gt; &lt;p&gt;Let's go through all the parts of a Camel integration and see them in more detail through an example. The source code of the example is &lt;a href="https://github.com/jboss-fuse/camel-quarkus-examples/tree/camel-quarkus-examples-1.6.0-product/file-bindy-ftp"&gt;available on GitHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Say that you need to process CSV files stored in a local directory by an external system. Those files contain book records consisting of a book title, author name, and genre. You want to split the records into separate CSV files by genre and store them on a remote SFTP server.&lt;/p&gt; &lt;p&gt;To accomplish this project, define a few Camel routes. The first route is ancillary, to simulate the external system that produces the CSV files containing the book data:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;import org.apache.camel.Exchange; import org.apache.camel.builder.RouteBuilder; import org.apache.camel.model.dataformat.BindyType; import org.apache.camel.processor.aggregate.GroupedBodyAggregationStrategy; public class Routes extends RouteBuilder { @Override public void configure() throws Exception { // Route 1: Generate some book objects with random data from("timer:generateBooks?period={{timer.period}}&amp;delay={{timer.delay}}") .log("Generating randomized books CSV data") .process("bookGenerator") // Marshal each book to CSV format .marshal().bindy(BindyType.Csv, Book.class) // Write CSV data to file .to("file:{{csv.location}}"); // More route definitions come here... } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The second route parses the discovered CSV files into a list of plain old Java objects (POJOs) using &lt;a href="https://camel.apache.org/components/latest/dataformats/bindy-dataformat.html"&gt;Camel Bindy&lt;/a&gt;. That list is split into individual &lt;code&gt;Book&lt;/code&gt; objects via &lt;code&gt;split(body())&lt;/code&gt; and passed to another Camel endpoint called &lt;code&gt;direct:aggregateBooks&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; ... // Route 2: Consume book CSV files from("file:{{csv.location}}?delay=1000") .log("Reading books CSV data from ${header.CamelFileName}") .unmarshal().bindy(BindyType.Csv, Book.class) .split(body()) .to("direct:aggregateBooks"); // More route definitions come here...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The third route picks the &lt;code&gt;Book&lt;/code&gt; objects produced by the previous route, does the aggregation (producing a list of &lt;code&gt;Book&lt;/code&gt;s per genre), and passes them to the last route:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; ... // Route 3: Aggregate books based on their genre from("direct:aggregateBooks") .setHeader("BookGenre", simple("${body.genre}")) .aggregate(simple("${body.genre}"), new GroupedBodyAggregationStrategy()).completionInterval(5000) .log("Processed ${header.CamelAggregatedSize} books for genre '${header.BookGenre}'") .to("seda:processed"); // One more route definition comes here...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The fourth route serializes aggregated lists of &lt;code&gt;Book&lt;/code&gt; records back to CSV and stores them on a remote SFTP server:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; ... // Route 4: Marshal books back to CSV format from("seda:processed") .marshal().bindy(BindyType.Csv, Book.class) .setHeader(Exchange.FILE_NAME, simple("books-${header.BookGenre}-${exchangeId}.csv")) // Send aggregated book genre CSV files to an FTP host .to("sftp://{{ftp.username}}@{{ftp.host}}:{{ftp.port}}/uploads/books?password={{ftp.password}}") .log("Uploaded ${header.CamelFileName}");&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;The pom.xml file&lt;/h2&gt; &lt;p&gt;For a plain Camel project, you would have to list the Camel component artifacts (&lt;code&gt;camel-bindy&lt;/code&gt;, &lt;code&gt;camel-seda&lt;/code&gt;, etc.) as dependencies in your &lt;code&gt;pom.xml&lt;/code&gt; file. For a Camel Quarkus project, you'll have to list their Camel Quarkus counterparts (&lt;code&gt;camel-quarkus-bindy&lt;/code&gt;, &lt;code&gt;camel-quarkus-seda&lt;/code&gt;, etc.). Import &lt;code&gt;org.apache.camel.quarkus:camel-quarkus-bom&lt;/code&gt; to manage the versions of the dependencies:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;project&gt; ... &lt;properties&gt; &lt;camel-quarkus.version&gt;1.6.0.fuse-jdk11-800006-redhat-00001&lt;/camel-quarkus.version&gt; &lt;quarkus.version&gt;1.11.6.Final-redhat-00001&lt;/quarkus.version&gt; ... &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- Import BOM --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel.quarkus&lt;/groupId&gt; &lt;artifactId&gt;camel-quarkus-bom&lt;/artifactId&gt; &lt;version&gt;${camel-quarkus.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel.quarkus&lt;/groupId&gt; &lt;artifactId&gt;camel-quarkus-bean&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel.quarkus&lt;/groupId&gt; &lt;artifactId&gt;camel-quarkus-bindy&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel.quarkus&lt;/groupId&gt; &lt;artifactId&gt;camel-quarkus-direct&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel.quarkus&lt;/groupId&gt; &lt;artifactId&gt;camel-quarkus-file&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel.quarkus&lt;/groupId&gt; &lt;artifactId&gt;camel-quarkus-ftp&lt;/artifactId&gt; &lt;/dependency&gt; ... &lt;/dependencies&gt; ... &lt;/project&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Runtime prerequisites&lt;/h2&gt; &lt;p&gt;To run the example, you need an SFTP server. For testing, you can use a Docker container as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker run -ti --rm -p 2222:2222 \ -e PASSWORD_ACCESS=true \ -e USER_NAME=ftpuser \ -e USER_PASSWORD=ftppassword \ -e DOCKER_MODS=linuxserver/mods:openssh-server-openssh-client \ linuxserver/openssh-server&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Quarkus dev mode&lt;/h2&gt; &lt;p&gt;Having all that in place, you can build the project and start Quarkus in &lt;code&gt;dev&lt;/code&gt; mode:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mvn clean compile quarkus:dev&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This mode lets the Quarkus tooling watch for changes in your workspace and recompile and redeploy the application upon any change. Please refer to the &lt;a href="https://camel.apache.org/camel-quarkus/latest/first-steps.html#_development_mode"&gt;development mode section of the Camel Quarkus user guide&lt;/a&gt; for more details.&lt;/p&gt; &lt;p&gt;You should start to see log messages appearing on the console, like the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;[route1] (Camel (camel-1) thread #3 - timer://generateBooks) Generating randomized books CSV data [route2] (Camel (camel-1) thread #1 - file:///tmp/books) Reading books CSV data from 89A0EE24CB03A69-0000000000000000 [route3] (Camel (camel-1) thread #0 - AggregateTimeoutChecker) Processed 34 books for genre 'Action' [route3] (Camel (camel-1) thread #0 - AggregateTimeoutChecker) Processed 31 books for genre 'Crime' [route3] (Camel (camel-1) thread #0 - AggregateTimeoutChecker) Processed 35 books for genre 'Horror'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can try changing something in &lt;code&gt;Routes.java&lt;/code&gt; and see that the application gets live-reloaded after saving the file.&lt;/p&gt; &lt;h2&gt;Packaging and running the application&lt;/h2&gt; &lt;p&gt;Once you are done with development, you can package and run the application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mvn clean package -DskipTests $ java -jar target/*-runner.jar&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Deploying to OpenShift&lt;/h2&gt; &lt;p&gt;To deploy the application to OpenShift, run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mvn clean package -DskipTests -Dquarkus.kubernetes.deploy=true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check that the pods are running:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get pods NAME READY STATUS RESTARTS AGE camel-quarkus-examples-file-bindy-ftp-5d48f4d85c-sjl8k 1/1 Running 0 21s ssh-server-deployment-5c667bccfc-52xfz 1/1 Running 0 21s&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tail the application logs, and you should see messages similar to those in &lt;code&gt;dev&lt;/code&gt; mode:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc logs -f camel-quarkus-examples-file-bindy-ftp-5d48f4d85c-sjl8k&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;About Camel Quarkus technology preview&lt;/h2&gt; &lt;p&gt;Camel Quarkus is available as a technology preview (TP) component in &lt;a href="https://developers.redhat.com/integration"&gt;Red Hat Integration 2021 Q2&lt;/a&gt;. Technology preview features provide early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process. As we move towards general availability (GA) later this year, each preview release will focus on key use cases.&lt;/p&gt; &lt;p&gt;The following Quarkus extensions are included in this technology preview:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;camel-quarkus-bean&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-quarkus-bindy&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-quarkus-core&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-quarkus-direct&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-quarkus-file&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-quarkus-ftp&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-quarkus-log&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-quarkus-main&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-quarkus-microprofile-health&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-quarkus-mock&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-quarkus-seda&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-quarkus-timer&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;These extensions are supported in JVM mode only.&lt;/p&gt; &lt;p&gt;Note that more Camel Quarkus extensions are provided by the &lt;a href="https://camel.apache.org/camel-quarkus/latest/reference/index.html"&gt;Apache Camel&lt;/a&gt; community. These extensions can be combined with the extensions provided by Red Hat Integration.&lt;/p&gt; &lt;p&gt;For more details, please refer to the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_integration/2021.q2/html/release_notes_for_red_hat_integration_2021.q2/camel-quarkus-relnotes_integration"&gt;release notes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Your feedback is welcome via the &lt;a href="https://access.redhat.com/support"&gt;Red Hat Support Portal&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/05/17/integrating-systems-apache-camel-and-quarkus-red-hat-openshift" title="Integrating systems with Apache Camel and Quarkus on Red Hat OpenShift"&gt;Integrating systems with Apache Camel and Quarkus on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/UlMs1CUpvjY" height="1" width="1" alt=""/&gt;</summary><dc:creator>Peter Palaga</dc:creator><dc:date>2021-05-18T03:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/05/17/integrating-systems-apache-camel-and-quarkus-red-hat-openshift</feedburner:origLink></entry><entry><title>Introduction to the Node.js reference architecture, Part 3: Code consistency</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/YEUIpTNoNyE/introduction-nodejs-reference-architecture-part-3-code-consistency" /><author><name>Lucas Holmquist</name></author><id>cfa54527-158a-46bc-a33d-9768f9a42b70</id><updated>2021-05-17T03:00:00Z</updated><published>2021-05-17T03:00:00Z</published><summary type="html">&lt;p&gt;Welcome back to our ongoing series about the &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; reference architecture. &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/"&gt;Part 1&lt;/a&gt; introduced what the Node.js reference architecture is all about, and &lt;a href="https://developer.ibm.com/languages/node-js/blogs/nodejs-reference-architectire-pino-for-logging/"&gt;Part 2&lt;/a&gt; took a look at logging. In this article, we will dive into code consistency and how to enforce it with a linter tool like ESLint.&lt;/p&gt; &lt;h2&gt;Why code consistency matters&lt;/h2&gt; &lt;p&gt;One critical aspect of working on JavaScript projects effectively as a team is having consistency in the formatting of your code. This ensures that when different team members collaborate on the shared codebase, they know what coding patterns to expect, allowing them to work more efficiently. A lack of consistency increases the learning curve for developers and can potentially detract from the main project goal.&lt;/p&gt; &lt;p&gt;When the Node.js teams at Red Hat and IBM started the discussion on code consistency, it quickly became apparent that this is an area where people have strong opinions, and one size does not fit all. It's amazing how much time you can spend talking about the right place for a bracket!&lt;/p&gt; &lt;p&gt;The one thing we could agree on, though, is the importance of using a consistent style within a project and enforcing it through automation.&lt;/p&gt; &lt;h2&gt;ESLint&lt;/h2&gt; &lt;p&gt;In surveying the tools used across Red Hat and IBM to check and enforce code consistency, &lt;a href="https://eslint.org/"&gt;ESLint&lt;/a&gt; quickly surfaced as the most popular choice. This configurable linter tool analyzes code to identify JavaScript patterns and maintain quality.&lt;/p&gt; &lt;p&gt;While we found that different teams used different code styles, many of them reported that they used ESLint to get the job done. ESLint is an &lt;a href="https://developers.redhat.com/topics/open-source/"&gt;open source&lt;/a&gt; project hosted by the &lt;a href="https://openjsf.org/"&gt;OpenJS Foundation&lt;/a&gt;, confirming it as a solid choice with open governance. We know we'll always have the opportunity to contribute fixes and get involved with the project.&lt;/p&gt; &lt;p&gt;ESLint comes with many pre-existing code style configurations that you can easily add to your projects. Using one of these shareable configurations has many benefits. By using an existing config, you can avoid "reinventing the wheel"; someone else has probably already created the configuration you are looking for. Another advantage is that new team members (or open source contributors) might already be familiar with the config you are using, enabling them to get up to speed more quickly.&lt;/p&gt; &lt;p&gt;Here are a few common configurations to help you get started:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.npmjs.com/package/eslint-config-airbnb-standard"&gt;&lt;code&gt;eslint-config-airbnb-standard&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.npmjs.com/package/eslint-config-semistandard"&gt;&lt;code&gt;eslint-config-semistandard&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.npmjs.com/package/eslint-config-standard"&gt;&lt;code&gt;eslint-config-standard&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/prettier/prettier-eslint"&gt;&lt;code&gt;eslint-config-prettier&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;A complete list can be found on npmjs.org &lt;a href="https://www.npmjs.com/search?q=eslint-config-&amp;ranking=popularity"&gt;using this query&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Note we do not recommend any particular code style or ESLint config. It's more important that you choose one standard and that you apply it consistently across your organization. If that is not possible, then you should at least make sure it's used consistently across related projects.&lt;/p&gt; &lt;p&gt;At this point, I must admit we did not really spend &lt;em&gt;that&lt;/em&gt; much time talking about where the brackets should go. But that is one of the reasons we suggest looking at one of the existing configs: Adopting existing best practices saves a lot of time (and arguments) so you can spend that time coding instead.&lt;/p&gt; &lt;h3&gt;Adding ESLint to your Node.js project&lt;/h3&gt; &lt;p&gt;Based on the advice in the reference architecture, the Red Hat Node.js team recently updated the &lt;a href="https://github.com/nodeshift"&gt;NodeShift project&lt;/a&gt; to use ESLint.&lt;/p&gt; &lt;p&gt;Adding ESLint to your project is a pretty straightforward process. In fact, ESLint has a wizard that you can run on the command line interface to get you started. You can run:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ npx eslint --init &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and then follow the prompts. This post won’t go into the specifics of the &lt;code&gt;init&lt;/code&gt; wizard, but you can find more information in the &lt;a href="https://eslint.org/docs/user-guide/getting-started"&gt;ESLint documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Our team likes using semi-colons, so we decided to use the &lt;a href="https://www.npmjs.com/package/eslint-config-semistandard"&gt;&lt;code&gt;semistandard&lt;/code&gt; config&lt;/a&gt;. It was easy to install by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ npx install-peerdeps --dev eslint-config-semistandard&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, in our &lt;a href="https://github.com/nodeshift/nodeshift/blob/main/.eslintrc.json#L2"&gt;&lt;code&gt;.eslintrc.json&lt;/code&gt;&lt;/a&gt; file, we made sure to extend &lt;code&gt;semistandard&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;{ "extends": "semistandard", "rules": { "prefer-const": "error", "block-scoped-var": "error", "prefer-template": "warn", "no-unneeded-ternary": "warn", "no-use-before-define": [ "error", "nofunc" ] } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You will notice that we have some custom rules set up as well. If you have custom rules for your project, this is where you should put them.&lt;/p&gt; &lt;h3&gt;Automating the code linter&lt;/h3&gt; &lt;p&gt;Having a linter in place is great, but it is only effective if you run it. While you can run the &lt;code&gt;eslint&lt;/code&gt; command manually to check your code consistency, remembering to run it that way can become burdensome and error-prone. The best approach is to set up some type of automation.&lt;/p&gt; &lt;p&gt;The first step is to create an npm script like &lt;code&gt;pretest&lt;/code&gt; that will make sure linting happens before your tests are run. That script might look something like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt; "scripts": { "pretest": "eslint --ignore-path .gitignore ." }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice that we are telling ESLint to ignore paths that are contained in our &lt;code&gt;.gitignore&lt;/code&gt; file, so make sure the &lt;code&gt;node_modules&lt;/code&gt; folder and other derived files are included in that ignore file. Using an npm script like this easily integrates into most continuous integration (CI) platforms.&lt;/p&gt; &lt;p&gt;Another alternative is to configure hooks so that the linter runs before the code is committed. Libraries like &lt;a href="https://www.npmjs.com/package/husky"&gt;Husky&lt;/a&gt; can help with this workflow. Just be sure that these precommit checks don't take too long, or your developers might complain.&lt;/p&gt; &lt;h3&gt;Conclusion&lt;/h3&gt; &lt;p&gt;It is critical to make sure you enforce consistent code standards across all your projects so that your team can collaborate efficiently. The best way to keep up with that task is to use a linter and automate it as part of your workflow. We recommend ESLint, but you are free to choose whatever tool you want—as long as you have something.&lt;/p&gt; &lt;p&gt;While you wait for the next installment in this series on the Node.js reference architecture, visit the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;GitHub project&lt;/a&gt; to explore sections that might be covered in future articles. If you want to learn more about what Red Hat is up to on the Node.js front, check out &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;our Node.js landing page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency" title="Introduction to the Node.js reference architecture, Part 3: Code consistency"&gt;Introduction to the Node.js reference architecture, Part 3: Code consistency&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/YEUIpTNoNyE" height="1" width="1" alt=""/&gt;</summary><dc:creator>Lucas Holmquist</dc:creator><dc:date>2021-05-17T03:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency</feedburner:origLink></entry><entry><title type="html">DevConf.US 2021 - Containers, OpenShift, architecture blueprints, and diagram tooling</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/KdVlenKGAPI/devconf-us-containers-openshift-architecture-blueprints-diagram-tooling.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/85iCFvAjG18/devconf-us-containers-openshift-architecture-blueprints-diagram-tooling.html</id><updated>2021-05-13T05:00:00Z</updated><content type="html">DevConf.US 2021 has kicked off their call for papers this last month and of course it will be a virtual event (hopefully for this one last time) hosted on September 2-3. It's the 4th annual, free, Red Hat sponsored technology conference for community project and professional contributors to Free and Open Source technologies coming to a web browser near you! There is no admission or ticket charge for DevConf.US events. However, you are required to complete a free registration. Talks, presentations and workshops will all be in English. I've put together the following collection of talks as my submissions and happy to preview them here with you. Installing OpenShift as Dev Locally in Minutes! Are you an AppDev that's truly tired of messing with Kubernetes and all those YAML files? Do you have a project in a Git repository and just want to use a container platform for deploying, testing, and running your applications while in the development phase locally on your own machine? Have we got the session for you! Join us for this power session showing you live how easy it is to install the OpenShift Container Platform (latest 4.x version) on your local developer machine in just minutes. Attendees will experience the installation and quick tour of OpenShift Container Platform from the AppDev point of view. Furthermore, they can take it all home as it's hosted in a project online with links and tips for projects that help them to learn how to leverage existing images and products from the provided image registry or Operator Hub catalogs. It's so easy, next year the attendees can return to this conference and share their latest AppDev projects running on their local machines too!  (25 min) Designing your best architecture diagrams  Diagraming is one of the most important communication tools for sharing your project and architectural ideas to your colleagues and teams. In this workshop attendees are walking step-by-step through using an open source tool we host online for designing architecture diagrams like an expert. Attendees work through the following: * open and explore the tooling in your favourite web browser * explore the provided asset libraries for drag-and-drop designing * learn about the three types of diagrams that make up a good design * create your first simple logical diagram * create your first simple schematic diagram * create a detailed diagram * how to export diagrams and elements from a diagram * design tips and tricks Each of the individual labs in this workshop are stand alone, allowing the attendee to focus on anything of interest without having to work through the previous labs. If you're looking to become more proficient in sharing your ideas, architectures, and projects visually to wider audiences you can't underestimate the value of a good diagram. Join us to learn the tips and tricks that make a good diagram such a good communication vehicle and how our tooling eases your design tasks. (workshop)  The workshop is fully implemented and tested with designers and architects around the world and fine tuned to the beginning designer. Workshop is for your previewing pleasure. Real life Retail Architectures in Action We've all had the retail shopping experience, either online or in a physical shop, but have you ever wanted to take a serious look at how they deliver that experience in a cost effective manner at scale? This session takes attendees on a tour of three architecture blueprints covering three of the most interesting solutions retail organisations have to implement successfully to survive. Not only are these architecture solutions interesting, but they are based on successful real life implementations featuring open source technologies and power a lot of your world wide shopping experiences. The following three use cases will be discussed and detailed in architectural diagrams showcasing how open technologies are integrated to solve them: * Supply chain integration * Real-time stock control * Retail data framework The attendee shall depart this session with a working knowledge of how to map general open source technologies to their solutions with examples all based on real life use cases. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution spaces as they can be pre-loaded into our architecture diagram tooling for modification.  Furthermore, content is available online (for example: https://dzone.com/articles/supply-chain-integration-an-architectural-introduc) for each of these use cases providing attendees with reference material post conference. (25 min) I'll keep you posted if these are accepted and share the planning as we move towards this event in the September.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/KdVlenKGAPI" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/85iCFvAjG18/devconf-us-containers-openshift-architecture-blueprints-diagram-tooling.html</feedburner:origLink></entry></feed>
